# ============================================================================
# SIMPLE LLM INFERENCE CONFIGURATION
# ============================================================================
# Basic configuration for running the simple LLM inference workload
# ============================================================================

# Execution mode
--execution_mode=benchmark

# Workload and worker profiles
--workload_profile_path=./profiles/workload/llm_inference_workload.yaml
--worker_profile_path=./profiles/workers/llm_simple_workers.yaml

# Scheduler configuration
--scheduler=tetrisched
--scheduler_runtime=100
--time_discretization=1

# Scheduling policies
--enforce_deadlines=True
--retract_schedules=False
--release_taskgraphs=True

# Logging configuration
--log_dir=./logs
--log_file_name=llm_simple.log
--log_level=info

# TetriSched parameters
--goal=max_goodput
--plan_ahead=-1

# Output
--csv_file_name=llm_simple_results.csv

